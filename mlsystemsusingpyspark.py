# -*- coding: utf-8 -*-
"""MLSystemsUsingPyspark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GeZ9ZLqCoceZ57QRSsZdwce-RLwxNWV5
"""

pip install pyspark

import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import pandas as pd
import pyspark
from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F
from pyspark import SparkFiles
import matplotlib.pyplot as plt

spark = SparkSession \
       .builder \
       .appName("Week8_Assignment") \
       .getOrCreate()

spark

spark=SparkSession.builder.getOrCreate()

"""## 1. Read the Data"""

EmpDF = spark.read.csv(SparkFiles.get("/content/download.csv"),inferSchema=True, header= True)
EmpDF.show()

"""### 1.1 Display the number of rows and columns in the dataset"""

print("Number of Rows: ", EmpDF.count())
print("Number of Columns", len(EmpDF.columns))

"""### 1.2 Check the datatype of the variables"""

EmpDF.cache()
EmpDF.printSchema()

"""### 2. Convert the incorrect column type into its suitable column type. And drop the redundant features"""

from pyspark.sql.types import StringType,IntegerType
EmpDF = EmpDF.withColumn("Job Family Code",EmpDF["Job Family Code"].cast(IntegerType()))
EmpDF = EmpDF.withColumn("Job Code",EmpDF["Job Code"].cast(IntegerType()))
EmpDF = EmpDF.withColumn("Employee Identifier",EmpDF["Employee Identifier"].cast(StringType()))

#EmpDF = EmpDF.drop('Organization Group')
EmpDF = EmpDF.drop('Department')
EmpDF = EmpDF.drop('Union')
EmpDF = EmpDF.drop('Job Family')
#EmpDF = EmpDF.drop('Job')

EmpDF.describe().toPandas().transpose()

"""### 3. Check basic statistics and perform necessary data preprocessing (Like removing negative amount)"""

from pyspark.sql.functions import col,isnan,when,count
EmpDF_shw = EmpDF.select([count(when(col(c).contains('None') | \
                            col(c).contains('NULL') | \
                            (col(c) == '' ) | \
                            col(c).isNull() | \
                            isnan(c), c 
                           )).alias(c)
                    for c in EmpDF.columns])
EmpDF_shw.show()

"""### 4. Perform Missing Value Analysis"""

EmpDF.groupBy('Job Family Code').count().show()

EmpDF.groupBy('Union Code').count().show()

EmpDF.groupBy('Job Code').count().show()

distinctDF = EmpDF.distinct()
print("Distinct count: "+str(distinctDF.count()))
distinctDF.show(truncate=False)

EmpDF3 = EmpDF.dropDuplicates()
print("Distinct count: "+str(EmpDF3.count()))
EmpDF3.show(truncate=False)

from pyspark.sql import functions as F

EmpDF = EmpDF.withColumn(
    "Union Code",
    F.when(EmpDF["Union Code"] < 0, 0).when(F.col("Union Code").isNull(), 253).otherwise(F.col("Union Code")),
)
EmpDF = EmpDF.withColumn(
    "Job Family Code",
    F.when(EmpDF["Job Family Code"] < 0, 0).when(F.col("Job Family Code").isNull(), 9900).otherwise(F.col("Job Family Code")),
)
EmpDF= EmpDF.withColumn(
    "Job Code",
    F.when(EmpDF["Job Code"] < 0, 0).when(F.col("Job Code").isNull(), 9775).otherwise(F.col("Job Code")),
)
EmpDF = EmpDF.withColumn(
    "Salaries",
    F.when(EmpDF["Salaries"] < 0, 0).when(F.col("Salaries").isNull(), 0).otherwise(F.col("Salaries")),
)
EmpDF = EmpDF.withColumn(
    "Overtime",
    F.when(EmpDF["Overtime"] < 0, 0).when(F.col("Overtime").isNull(), 0).otherwise(F.col("Overtime")),
)
EmpDF = EmpDF.withColumn(
    "Other Salaries",
    F.when(EmpDF["Other Salaries"] < 0, 0).when(F.col("Other Salaries").isNull(), 0).otherwise(F.col("Other Salaries")),
)
EmpDF = EmpDF.withColumn(
    "Total Salary",
    F.when(EmpDF["Total Salary"] < 0, 0).when(F.col("Total Salary").isNull(), 0).otherwise(F.col("Total Salary")),
)
EmpDF = EmpDF.withColumn(
    "Retirement",
    F.when(EmpDF["Retirement"] < 0, 0).when(F.col("Retirement").isNull(), 0).otherwise(F.col("Retirement")),
)
EmpDF= EmpDF.withColumn(
    "Health/Dental",
    F.when(EmpDF["Health/Dental"] < 0, 0).when(F.col("Health/Dental").isNull(), 0).otherwise(F.col("Health/Dental")),
)
EmpDF = EmpDF.withColumn(
    "Other Benefits",
    F.when(EmpDF["Other Benefits"] < 0, 0).when(F.col("Other Benefits").isNull(), 0).otherwise(F.col("Other Benefits")),
)
EmpDF = EmpDF.withColumn(
    "Total Benefits",
    F.when(EmpDF["Total Benefits"] < 0, 0).when(F.col("Total Benefits").isNull(), 0).otherwise(F.col("Total Benefits")),
)
EmpDF = EmpDF.withColumn(
    "Total Compensation",
    F.when(EmpDF["Total Compensation"] < 0, 0).when(F.col("Total Compensation").isNull(), 0).otherwise(F.col("Total Compensation")),
)

EmpDF.describe().toPandas().transpose()

from pyspark.sql.functions import col,isnan,when,count
EmpDF_shw = EmpDF.select([count(when(col(c).contains('None') | \
                            col(c).contains('NULL') | \
                            (col(c) == '' ) | \
                            col(c).isNull() | \
                            isnan(c), c 
                           )).alias(c)
                    for c in EmpDF.columns])
EmpDF_shw.show()

"""### 5. Exploratory Data Analysis

#### 5.1. Find top compensating organizations. Display using bar plot
"""

from pyspark.sql.functions import desc
fig = plt.figure(figsize =(25, 10))

Final_DF_bar = EmpDF[['Organization Group','Total Compensation']].groupby('Organization Group').sum('Total Compensation').sort(desc("sum(Total Compensation)")).toPandas().head(10)
# display the top 10 organisation group 
plt.bar(Final_DF_bar["Organization Group"], Final_DF_bar["sum(Total Compensation)"])

"""#### 5.2. Find top Compensating Jobs. Display using bar plot"""

from pyspark.sql.functions import desc
fig = plt.figure(figsize =(25, 10))

Final_DF_bar = EmpDF[['Job','Total Compensation']].groupby('Job').sum('Total Compensation').sort(desc("sum(Total Compensation)")).toPandas().head(15)
# display the top 10 organisation group 
plt.bar(Final_DF_bar["Job"], Final_DF_bar["sum(Total Compensation)"])

"""#### 5.3. Check Correlation of Target Variable with Other Independent Variables. Plot Heatmap"""

data = EmpDF.toPandas()

plt.figure(figsize=(8, 6))
sns.boxplot(x='Organization Group Code', y='Total Compensation', data=data, palette='GnBu')
title = plt.title('House Price by Garage Size')
sns.despine()

IndependentVar = ['Year','Organization Group Code','Department Code','Union Code','Job Family Code','Job Code','Salaries','Overtime','Other Salaries','Total Salary','Retirement','Health/Dental','Other Benefits','Total Benefits']
TargetVar =['TotalCompensation']

continuous_df = data[IndependentVar+ ["Total Compensation"]]
correlation = continuous_df.corr()
fig = plt.figure(figsize=(18,15))
ax = fig.add_subplot(111)
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(correlation, 
           xticklabels=correlation.columns.values,
           yticklabels=correlation.index.values,
           cmap=cmap ,annot=True)
ax.xaxis.tick_top()
plt.setp(ax.get_xticklabels(), rotation=90)
plt.show()

"""### 6. Perform necessary data pre-processing and divide the data into train and test set

### 6.1 Categorise the attributes into its type (Use one hot encoding wherever required)
"""

import pandas as pd
from pandas.plotting import scatter_matrix
numeric_features = [t[0] for t in EmpDF.dtypes if t[1] == 'int' or t[1] == 'double']
sampled_data = EmpDF.select(numeric_features).sample(False, 0.8).toPandas()
axs = scatter_matrix(sampled_data, figsize=(10, 10))
n = len(sampled_data.columns)
for i in range(n):
    v = axs[i, 0]
    v.yaxis.label.set_rotation(0)
    v.yaxis.label.set_ha('right')
    v.set_yticks(())
    h = axs[n-1, i]
    h.xaxis.label.set_rotation(90)
    h.set_xticks(())

import six
for i in EmpDF.columns:
    if not( isinstance(EmpDF.select(i).take(1)[0][0], six.string_types)):
        print( "Correlation to Total Compensation for ", i, EmpDF.stat.corr('Total Compensation',i))

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = ['Year','Organization Group Code','Union Code','Job Family Code','Job Code','Salaries','Overtime','Other Salaries','Total Salary','Retirement','Health/Dental','Other Benefits','Total Benefits'], outputCol = 'features')
vemp_df = vectorAssembler.transform(EmpDF)
vemp_df = vemp_df.select(['features', 'Total Compensation'])
vemp_df.show(3)

"""### 6.2 Split the data into train and test set"""

splits = vemp_df.randomSplit([0.7, 0.3])
train_df = splits[0]
test_df = splits[1]

"""### 7. Fit Linear Regression model on the data and check its performance"""

from pyspark.ml.regression import LinearRegression
lr = LinearRegression(featuresCol = 'features', labelCol='Total Compensation', maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_df)
print("Coefficients: " + str(lr_model.coefficients))
print("Intercept: " + str(lr_model.intercept))

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

train_df.describe().show()

lr_predictions = lr_model.transform(test_df)
lr_predictions.select("prediction","Total Compensation","features").show(50)
from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="Total Compensation",metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))

test_result = lr_model.evaluate(test_df)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)

print("numIterations: %d" % trainingSummary.totalIterations)
print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
trainingSummary.residuals.show()

predictions = lr_model.transform(test_df)
predictions.select("prediction","Total Compensation","features").show()

"""### 8. Fit Decision Tree Regression model on the data and check its performance (Optional)"""

from pyspark.ml.regression import DecisionTreeRegressor
dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'Total Compensation')
dt_model = dt.fit(train_df)
dt_predictions = dt_model.transform(test_df)
dt_evaluator = RegressionEvaluator(
    labelCol="Total Compensation", predictionCol="prediction", metricName="rmse")
rmse = dt_evaluator.evaluate(dt_predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

dt_model.featureImportances

EmpDF.take(1)

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/Week_5_8_Assignment_Part2.ipynb

